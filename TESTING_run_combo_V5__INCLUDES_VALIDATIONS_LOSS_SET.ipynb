{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1237f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "\n",
    "from Equations_Run_Combo_V_2 import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a84588",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# READ READ \n",
    "\n",
    "# THIS Was written to include a valudation set so that we can see how the model loss is generalizing by comapring train and VALloss loss \n",
    "# values ... However i am not convinced that this adds to the procedure more so thatn the current set up\n",
    "\n",
    "\n",
    "# If included later, all helper functions below must be added as well as there are minor changes as well ... \n",
    "\n",
    "############# READ READ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e588525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_loss_BCE_THRESH_AND_SEVERITY_PENALIZATION(preds, actuals, ACTUALS_RAW, balancing_Weight_factor , use_LOW_weights : bool):\n",
    "    probs = torch.sigmoid(preds)\n",
    "    \n",
    "    thresh_bucket_1 = (0.7, 0.8); thresh_bucket_1_factor = 1.3 if use_LOW_weights else 1.5\n",
    "    thresh_bucket_2 = (0.8, 0.9); thresh_bucket_2_factor = 1.7 if use_LOW_weights else 2.0\n",
    "    thresh_bucket_3 = (0.9, 1.0); thresh_bucket_3_factor = 2.5 if use_LOW_weights else 3.0\n",
    "\n",
    "    SEVERE_CASE_VAL = 0.10\n",
    "    SEVERE_CASE_FACTOR = 1.5 \n",
    "    # SEVERE_CASE_FACTOR = 2.5 \n",
    "\n",
    "\n",
    "    weights = []\n",
    "    \n",
    "    # Ensure actuals and probs are the same shape and flattened\n",
    "    actuals_flat = actuals.view(-1)\n",
    "    probs_flat = probs.view(-1)\n",
    "    ACTUALS_RAW_flat = ACTUALS_RAW.view(-1)\n",
    "\n",
    "    for a, a_raw, p in zip(actuals_flat, ACTUALS_RAW_flat, probs_flat):\n",
    "        a_val = a.item()  # Convert tensor to Python scalar\n",
    "        p_val = p.item()\n",
    "        a_raw_val = a_raw.item()\n",
    "        \n",
    "        if (a_val > 0.5) and (p_val >= 0.5):\n",
    "            weights.append(1.0)\n",
    "        elif (a_val > 0.5) and (p_val < 0.5):\n",
    "            weights.append(balancing_Weight_factor)  \n",
    "        elif (a_val < 0.5) and (p_val < 0.5):\n",
    "            weights.append(1.0)\n",
    "        elif (a_val < 0.5) and (p_val >= 0.5):\n",
    "            if (thresh_bucket_1[0] <= p_val < thresh_bucket_1[1]):\n",
    "                weights.append(thresh_bucket_1_factor)\n",
    "            elif (thresh_bucket_2[0] <= p_val < thresh_bucket_2[1]):\n",
    "                weights.append(thresh_bucket_2_factor)\n",
    "            elif (thresh_bucket_3[0] <= p_val <= thresh_bucket_3[1]) and (a_raw_val >= SEVERE_CASE_VAL):\n",
    "                weights.append(thresh_bucket_3_factor * SEVERE_CASE_FACTOR)  # Increase weight for severe cases\n",
    "            else:\n",
    "                weights.append(1.0) # Default weight if none of the above conditions are met\n",
    "\n",
    "    # Create weights tensor on the same device as preds\n",
    "    weights_tensor = torch.tensor(weights, dtype=torch.float32, device=preds.device).detach()\n",
    "    \n",
    "    # Reshape weights to match the original shape if needed\n",
    "    weights_tensor = weights_tensor.view_as(actuals)\n",
    "    \n",
    "    loss = F.binary_cross_entropy_with_logits(preds, actuals, weight=weights_tensor)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def custom_loss_BCE_THRESH_PENALIZATION(preds, actuals, balancing_Weight_factor , use_LOW_weights : bool):\n",
    "    probs = torch.sigmoid(preds)\n",
    "    \n",
    "    thresh_bucket_1 = (0.7, 0.8); thresh_bucket_1_factor = 1.3 if use_LOW_weights else 1.5\n",
    "    thresh_bucket_2 = (0.8, 0.9); thresh_bucket_2_factor = 1.7 if use_LOW_weights else 2.0 \n",
    "    thresh_bucket_3 = (0.9, 1.0); thresh_bucket_3_factor = 2.5 if use_LOW_weights else 3.0\n",
    "\n",
    "    weights = []\n",
    "    \n",
    "    # Ensure actuals and probs are the same shape and flattened\n",
    "    actuals_flat = actuals.view(-1)\n",
    "    probs_flat = probs.view(-1)\n",
    "    \n",
    "    for a, p in zip(actuals_flat, probs_flat):\n",
    "        a_val = a.item()  # Convert tensor to Python scalar\n",
    "        p_val = p.item()\n",
    "        \n",
    "        if (a_val > 0.5) and (p_val >= 0.5):\n",
    "            weights.append(1.0)\n",
    "        elif (a_val > 0.5) and (p_val < 0.5):\n",
    "            weights.append(balancing_Weight_factor)  \n",
    "        elif (a_val < 0.5) and (p_val < 0.5):\n",
    "            weights.append(1.0)\n",
    "        elif (a_val < 0.5) and (p_val >= 0.5):\n",
    "            if (thresh_bucket_1[0] <= p_val < thresh_bucket_1[1]):\n",
    "                weights.append(thresh_bucket_1_factor)\n",
    "            elif (thresh_bucket_2[0] <= p_val < thresh_bucket_2[1]):\n",
    "                weights.append(thresh_bucket_2_factor)\n",
    "            elif (thresh_bucket_3[0] <= p_val <= thresh_bucket_3[1]):\n",
    "                weights.append(thresh_bucket_3_factor)\n",
    "            else:\n",
    "                weights.append(1.0) # Default weight if none of the above conditions are met\n",
    "\n",
    "    # Create weights tensor on the same device as preds\n",
    "    weights_tensor = torch.tensor(weights, dtype=torch.float32, device=preds.device).detach()\n",
    "    \n",
    "    # Reshape weights to match the original shape if needed\n",
    "    weights_tensor = weights_tensor.view_as(actuals)\n",
    "    \n",
    "    loss = F.binary_cross_entropy_with_logits(preds, actuals, weight=weights_tensor)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "\n",
    "    \n",
    "    device = 'cpu'\n",
    "\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "        output = model(x_batch)\n",
    "        y_batch = y_batch.view(-1, 1) # change shape to (batch_size, 1) since output is (batch_size, 1)\n",
    "        loss = loss_function(output, y_batch)\n",
    "        loss_value = loss.item()\n",
    "        running_loss += loss_value\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_epoch_loss = running_loss / len(train_loader)\n",
    "    # train_losses.append(avg_epoch_loss)\n",
    "    return avg_epoch_loss\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, val_loader, loss_function):\n",
    "    \n",
    "    device = 'cpu'\n",
    "    model.train(False)\n",
    "    running_loss = 0.0\n",
    "    for batch_index, batch in enumerate(val_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(x_batch)\n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            loss = loss_function(output, y_batch)\n",
    "            running_loss += loss.item()\n",
    "    avg_loss_across_batches = running_loss / len(val_loader)\n",
    "    # val_losses.append(avg_loss_across_batches)\n",
    "    return avg_loss_across_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#*#*#*#* #*#*#*#* #*#*#*#* #*#*#*#* #*#*#*#*.     NEW NEW NEW SEPT 7\n",
    "\n",
    "\n",
    "def train_one_epoch_custom_loss_BCE_THRESH(model, train_loader, optimizer , \n",
    "                           balancing_Weight_factor , use_LOW_weights : bool):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    device = 'cpu'\n",
    "\n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "        output = model(x_batch)\n",
    "        y_batch = y_batch.view(-1, 1) # change shape to (batch_size, 1) since output is (batch_size, 1)\n",
    "        loss = custom_loss_BCE_THRESH_PENALIZATION(output, y_batch, balancing_Weight_factor , use_LOW_weights=use_LOW_weights)\n",
    "        loss_value = loss.item()\n",
    "        running_loss += loss_value\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_epoch_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    return avg_epoch_loss\n",
    "\n",
    "\n",
    "def validate_one_epoch_custom_loss_BCE_THRESH(model, val_loader , balancing_Weight_factor , use_LOW_weights : bool):\n",
    "    \n",
    "    device = 'cpu'\n",
    "    model.train(False)\n",
    "    running_loss = 0.0\n",
    "    for batch_index, batch in enumerate(val_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(x_batch)\n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            loss = custom_loss_BCE_THRESH_PENALIZATION(output, y_batch,  balancing_Weight_factor , use_LOW_weights=use_LOW_weights)\n",
    "            running_loss += loss.item()\n",
    "    avg_loss_across_batches = running_loss / len(val_loader)\n",
    "    # val_losses.append(avg_loss_across_batches)\n",
    "    return avg_loss_across_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch_custom_loss_BCE_THRESH_AND_SEVERITY(model, train_loader, train_loader_RAW_Y_vals, optimizer, \n",
    "                           balancing_Weight_factor , use_LOW_weights : bool):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    device = 'cpu'\n",
    "\n",
    "    for batch_index, (batch, batch_RAW_Y_vals) in enumerate(zip(train_loader, train_loader_RAW_Y_vals)):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "        x_batch_RAW, y_batch_RAW = batch_RAW_Y_vals[0].to(device), batch_RAW_Y_vals[1].to(device)\n",
    "        output = model(x_batch)\n",
    "        y_batch = y_batch.view(-1, 1) # change shape to (batch_size, 1) since output is (batch_size, 1)\n",
    "        y_batch_RAW = y_batch_RAW.view(-1, 1)\n",
    "\n",
    "\n",
    "        loss = custom_loss_BCE_THRESH_AND_SEVERITY_PENALIZATION(output, y_batch, ACTUALS_RAW=y_batch_RAW, balancing_Weight_factor=balancing_Weight_factor , \n",
    "                                                                use_LOW_weights=use_LOW_weights)\n",
    "        loss_value = loss.item()\n",
    "        running_loss += loss_value\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_epoch_loss = running_loss / len(train_loader)\n",
    "\n",
    "    return avg_epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "def validate_one_epoch_custom_loss_BCE_THRESH_AND_SEVERITY(model, val_loader ,train_loader_RAW_Y_vals ,  balancing_Weight_factor , use_LOW_weights : bool):\n",
    "    \n",
    "    device = 'cpu'\n",
    "    model.train(False)\n",
    "    running_loss = 0.0\n",
    "    for batch_index, batch in enumerate(val_loader):\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(x_batch)\n",
    "            y_batch = y_batch.view(-1, 1)\n",
    "            loss = custom_loss_BCE_THRESH_AND_SEVERITY_PENALIZATION(output, y_batch, balancing_Weight_factor , use_LOW_weights=use_LOW_weights)\n",
    "            running_loss += loss.item()\n",
    "    avg_loss_across_batches = running_loss / len(val_loader)\n",
    "    # val_losses.append(avg_loss_across_batches)\n",
    "    return avg_loss_across_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_combo_V_5(INDEX, combo, total_offset , use_print_acc_vs_pred : bool , pred_threshold_sigmoid01_up_bool : bool):   ##### include validation feature\n",
    "\n",
    "\n",
    "    def GRIDSEARCH_FUNCTION_WITH_CV_forTS(\n",
    "\n",
    "\n",
    "            use_USO_wticoncat_predictor_WEEKLY_END_MO : bool , use_UCO_wticoncat_predictor_WEEKLY_END_MO : bool ,\n",
    "            use_HUC_wticoncat_predictor_WEEKLY_END_MO : bool , use_HOD_wticoncat_predictor_WEEKLY_END_MO : bool ,\n",
    "            use_CRUD_wticoncat_predictor_WEEKLY_END_MO : bool , use_SCO_wticoncat_predictor_WEEKLY_END_MO : bool ,\n",
    "\n",
    "        learning_rate: float, num_epochs: int,\n",
    "        batch_size: int, use_bidirectional: bool,\n",
    "        lag: int, input_size: int,\n",
    "        hidden_size: int, num_layers: int,\n",
    "\n",
    "        use_monthly_dfs_only: bool,\n",
    "        \n",
    "        use_binary_0_1_retRate: bool,\n",
    "\n",
    "        use_binary_neg1_1: bool,\n",
    "        use_ret_rate: bool,\n",
    "        use_print_acc: bool,\n",
    "        use_dropout: bool,\n",
    "        # iter_per_valSET: int,\n",
    "        use_class_weighting: bool,\n",
    "        is_deterministic: bool,\n",
    "        seed_num: int,\n",
    "\n",
    "        use_existing_lagged_data : bool,\n",
    " \n",
    "        use_dynamic_weights : bool ,\n",
    "\n",
    "        use_binary_0_1_retRate_custom_neg : bool ,\n",
    "        use_binary_0_1_retRate_custom_pos : bool ,\n",
    "        binary_0_1_cutoff_ret_rate_percentage : float,  ### cutoff for the  use_binary_0_1_retRate_custom_pos ot use_binary_0_1_retRate_custom_neg\n",
    "\n",
    "\n",
    "        POS_weight_multiplier : float , \n",
    "        use_rolling_fixed_train_size : bool , \n",
    "        \n",
    "        use_existing_initial_weights : bool ,\n",
    "\n",
    "        state_dict ,\n",
    "\n",
    "                use_custom_loss_function_BCE_THRESH: bool, # NEW NEW NEW\n",
    "\n",
    "                use_custom_loss_function_BCE_THRESH_AND_SEVERITY: bool, # NEW NEW NEW\n",
    "\n",
    "                use_LOW_weights_for_BCE_custom_loss : bool, # NEW NEW NEW\n",
    "\n",
    "\n",
    "        train_start_month , # \"2005-02\",\n",
    "        val_start_month , # '2020-01' ,    # test_start_month = '2022-01' ; test_end_month = '2022-12' \n",
    "        val_end_month , # '2021-12' ,\n",
    "        num_preds_per_fold  , # should be 8 folds in the algo\n",
    "\n",
    "                pred_threshold_sigmoid01_up  = None  , # NEW NEW NEW\n",
    "\n",
    "        combo_index=INDEX  ### THIS IS THE INDEX in the parent function , it is passed there and does not need ot be explicitly stated when calling the GS func via parent function\n",
    "    ):\n",
    "\n",
    "\n",
    "######### --------------------------------------- DATA IMPORT ---------------------------------------\n",
    "\n",
    "        if use_existing_lagged_data: \n",
    "            if use_USO_wticoncat_predictor_WEEKLY_END_MO:\n",
    "                cache = lagged_cache[f\"outer_lag_{lag}\"]['USO_wticoncat_predictor_WEEKLY_END_MO'][f'lag_{lag}']\n",
    "                df_lagged_PREDICTOR_short_EXPLANATORY = cache[\"df_lagged_USO_wticoncat_predictor_WEEKLY_END_MO_short_EXPLANATORY\"]\n",
    "            if use_UCO_wticoncat_predictor_WEEKLY_END_MO:\n",
    "                cache = lagged_cache[f\"outer_lag_{lag}\"]['UCO_wticoncat_predictor_WEEKLY_END_MO'][f'lag_{lag}']\n",
    "                df_lagged_PREDICTOR_short_EXPLANATORY = cache[\"df_lagged_UCO_wticoncat_predictor_WEEKLY_END_MO_short_EXPLANATORY\"]\n",
    "            if use_HUC_wticoncat_predictor_WEEKLY_END_MO:\n",
    "                cache = lagged_cache[f\"outer_lag_{lag}\"]['HUC_wticoncat_predictor_WEEKLY_END_MO'][f'lag_{lag}']\n",
    "                df_lagged_PREDICTOR_short_EXPLANATORY = cache[\"df_lagged_HUC_wticoncat_predictor_WEEKLY_END_MO_short_EXPLANATORY\"]\n",
    "            if use_HOD_wticoncat_predictor_WEEKLY_END_MO:\n",
    "                cache = lagged_cache[f\"outer_lag_{lag}\"]['HOD_wticoncat_predictor_WEEKLY_END_MO'][f'lag_{lag}']\n",
    "                df_lagged_PREDICTOR_short_EXPLANATORY = cache[\"df_lagged_HOD_wticoncat_predictor_WEEKLY_END_MO_short_EXPLANATORY\"]\n",
    "            if use_CRUD_wticoncat_predictor_WEEKLY_END_MO:\n",
    "                cache = lagged_cache[f\"outer_lag_{lag}\"]['CRUD_wticoncat_predictor_WEEKLY_END_MO'][f'lag_{lag}']\n",
    "                df_lagged_PREDICTOR_short_EXPLANATORY = cache[\"df_lagged_CRUD_wticoncat_predictor_WEEKLY_END_MO_short_EXPLANATORY\"]\n",
    "            if use_SCO_wticoncat_predictor_WEEKLY_END_MO:\n",
    "                cache = lagged_cache[f\"outer_lag_{lag}\"]['SCO_wticoncat_predictor_WEEKLY_END_MO'][f'lag_{lag}']\n",
    "                df_lagged_PREDICTOR_short_EXPLANATORY = cache[\"df_lagged_SCO_wticoncat_predictor_WEEKLY_END_MO_short_EXPLANATORY\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ###### _____________ FROM PREV VERSION \n",
    "            #### MONTLY \n",
    "            df_lagged_US_energy_PPI = cache[\"df_lagged_US_energy_PPI\"] ; df_lagged_EU28_PPI = cache[\"df_lagged_EU28_PPI\"]\n",
    "            df_lagged_US_PMI = cache[\"df_lagged_US_PMI\"] ; df_lagged_oecd_pet_stocks = cache[\"df_lagged_oecd_pet_stocks\"]\n",
    "            #### MONTLY \n",
    "            #### WEEKLY\n",
    "            df_lagged_oecd_stocks_oilSPR_wkly = cache[\"df_lagged_oecd_stocks_oilSPR_wkly\"]\n",
    "            df_lagged_oecd_stocks_oilnonSPR_wkly = cache[\"df_lagged_oecd_stocks_oilnonSPR_wkly\"] ; df_lagged_spec = cache[\"df_lagged_spec\"]\n",
    "            df_lagged_wklyUSdollarIndex = cache[\"df_lagged_wklyUSdollarIndex\"] ; df_lagged_futures_3m_copper_weekly = cache[\"df_lagged_futures_3m_copper_weekly\"]\n",
    "            df_lagged_wti_crack_321 = cache[\"df_lagged_wti_crack_321\"] ; df_lagged_brent_crack_321 = cache[\"df_lagged_brent_crack_321\"]\n",
    "\n",
    "            # df_lagged_wti_weekly_y_short_EXPLANATORY = cache[\"df_lagged_wti_weekly_y_short_EXPLANATORY\"]\n",
    "\n",
    "            #### WEEKLY\n",
    "            ###### _____________ FROM PREV VERSION  \n",
    "            #**#*#*# LAGGED DFS FOR WEEKLY DFS TURNED INTO MONTHLY DFS ##### MONTHLY MONTHLY \n",
    "\n",
    "            # df_lagged_wti_monthly_y_short_EXPLANATORY = cache[\"df_lagged_wti_monthly_y_short_EXPLANATORY\"] \n",
    "            \n",
    "            df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo = cache[\"df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo\"]\n",
    "            df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo = cache[\"df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo\"] ; df_lagged_spec_monthly_wkTOmo = cache[\"df_lagged_spec_monthly_wkTOmo\"]\n",
    "            df_lagged_wklyUSdollarIndex_monthly_wkTOmo = cache[\"df_lagged_wklyUSdollarIndex_monthly_wkTOmo\"] ; df_lagged_futures_3m_copper_monthly_wkTOmo = cache[\"df_lagged_futures_3m_copper_monthly_wkTOmo\"]\n",
    "            df_lagged_wti_crack_321_monthly_wkTOmo = cache[\"df_lagged_wti_crack_321_monthly_wkTOmo\"] ; df_lagged_brent_crack_321_monthly_wkTOmo = cache[\"df_lagged_brent_crack_321_monthly_wkTOmo\"]\n",
    "            #**#*#*# LAGGED DFS FOR WEEKLY DFS TURNED INTO MONTHLY DFS ##### MONTHLY MONTHLY \n",
    "\n",
    "\n",
    "        lagged_dfs_monthly_only= [  #### MONTLY ONLY \n",
    "            df_lagged_US_energy_PPI[0],df_lagged_EU28_PPI[0],df_lagged_US_PMI[0],\n",
    "            df_lagged_oecd_pet_stocks[0], \n",
    "            \n",
    "            df_lagged_PREDICTOR_short_EXPLANATORY[0],\n",
    "            \n",
    "            df_lagged_oecd_stocks_oilSPR_monthly_wkTOmo[0],\n",
    "            df_lagged_oecd_stocks_oilnonSPR_monthly_wkTOmo[0],df_lagged_spec_monthly_wkTOmo[0],df_lagged_wklyUSdollarIndex_monthly_wkTOmo[0],\n",
    "            df_lagged_futures_3m_copper_monthly_wkTOmo[0],df_lagged_wti_crack_321_monthly_wkTOmo[0],df_lagged_brent_crack_321_monthly_wkTOmo[0],\n",
    "        ]\n",
    "\n",
    "        lagged_dfs_monthly_weekly = [ ### WEEKLY AND MONTHLY \n",
    "            df_lagged_US_energy_PPI[0],df_lagged_EU28_PPI[0],df_lagged_US_PMI[0],\n",
    "            df_lagged_oecd_pet_stocks[0],df_lagged_oecd_stocks_oilSPR_wkly[0],df_lagged_oecd_stocks_oilnonSPR_wkly[0],df_lagged_spec[0],\n",
    "            df_lagged_wklyUSdollarIndex[0],df_lagged_futures_3m_copper_weekly[0],df_lagged_wti_crack_321[0],df_lagged_brent_crack_321[0] ]\n",
    "            \n",
    "            # ,df_lagged_wti_monthly_y_short_EXPLANATORY[0] , \n",
    "             \n",
    "            # df_lagged_wti_weekly_y_short_EXPLANATORY[0] ] \n",
    "\n",
    "        lagged_df = lagged_dfs_monthly_only if use_monthly_dfs_only else lagged_dfs_monthly_weekly #*#* choose df based on input to function \n",
    "\n",
    "        ###### MONTHLY ONLY MERGE\n",
    "        df_merged = lagged_df[0].copy() # Use a copy to avoid modifying the original (pickled) DataFrame\n",
    "\n",
    "       #   IMPORTANT IMPORTANT                       --- Use copies of each DataFrame to avoid in-place modification of pickled data---                    ********* IMPORTANT IMPORTANT IMPORTANT \n",
    "        lagged_df_copies = [df.copy() for df in lagged_df[1:]]\n",
    "\n",
    "        for df in lagged_df_copies:\n",
    "            if 'predictor_value' in df.columns:\n",
    "                df.drop(columns='predictor_value', inplace=True)\n",
    "\n",
    "        for df in lagged_df_copies:\n",
    "            df_merged = pd.merge(df_merged, df, on='predictor_pred_date', how='inner')\n",
    "        df_merged = df_merged[2::].reset_index(drop=True) #**#*# first two vals are nans  \n",
    "        #   IMPORTANT IMPORTANT # Use copies of each DataFrame to avoid in-place modification of pickled data ********* IMPORTANT IMPORTANT IMPORTANT\n",
    "\n",
    "\n",
    "        tensor_formatted_data_full , predictor_data_wti_vals_full , prediction_date_full =  format_to_tensor(df_merged, lag_steps=lag ,target_col=\"predictor_value\", date_col=\"predictor_pred_date\" )\n",
    "\n",
    "        raw_actuals_full = predictor_data_wti_vals_full[:]  # same length as tensor_formatted_data\n",
    "\n",
    "        if use_binary_0_1_retRate : \n",
    "            predictor_data_wti_vals_full = [1 if val > 0 else 0 for val in predictor_data_wti_vals_full]\n",
    "        if use_binary_neg1_1 : \n",
    "            predictor_data_wti_vals_full = [1 if val > 0 else -1 for val in predictor_data_wti_vals_full]\n",
    "        if use_ret_rate : \n",
    "            predictor_data_wti_vals_full = predictor_data_wti_vals_full\n",
    "        if use_binary_0_1_retRate_custom_neg :             \n",
    "            predictor_data_wti_vals_full = [1 if val < - binary_0_1_cutoff_ret_rate_percentage else 0 for val in predictor_data_wti_vals_full] #  NEW\n",
    "        if use_binary_0_1_retRate_custom_pos :\n",
    "             predictor_data_wti_vals_full = [1 if val > binary_0_1_cutoff_ret_rate_percentage else 0 for val in predictor_data_wti_vals_full] #  NEW\n",
    "\n",
    "\n",
    "######### --------------------------------------- DATA LOADERS  - TRAIN/TEST/VAL ---------------------------------------\n",
    "\n",
    "        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        from torch.utils.data import DataLoader\n",
    "\n",
    "        prediction_date_full = pd.to_datetime(prediction_date_full)\n",
    "\n",
    "        # train_start_month = '2005-02' \n",
    "        # val_start_month = '2020-01' ; val_end_month = '2021-12'   # test_start_month = '2022-01' ; test_end_month = '2022-12' \n",
    "\n",
    "        idx_train_start_month = [idx for idx, date in enumerate(prediction_date_full) if str(date)[:7] == train_start_month][0]  ### this should be the first date of the train set\n",
    "        idx_val_start_month_full = [idx for idx, date in enumerate(prediction_date_full) if str(date)[:7] == val_start_month][0]  ### this hsould be the fist date of the val set\n",
    "\n",
    "        tensor_formatted_data = tensor_formatted_data_full[idx_train_start_month:] ### notice this has to be doen first for proper date handling \n",
    "        predictor_data_wti_vals = predictor_data_wti_vals_full[idx_train_start_month:]\n",
    "        prediction_date = prediction_date_full[idx_train_start_month:]\n",
    "        raw_actuals = raw_actuals_full[idx_train_start_month:]\n",
    "\n",
    "        # print(predictor_data_wti_vals)\n",
    "\n",
    "        idx_val_start_month = [idx for idx, date in enumerate(prediction_date) if str(date)[:7] == val_start_month][0]  ### this hsould be the fist date of the val set\n",
    "        idx_val_end_month   = [idx for idx, date in enumerate(prediction_date) if str(date)[:7] == val_end_month][0]\n",
    "\n",
    "        # num_preds_per_fold = 3  \n",
    "        num_folds = ((idx_val_end_month - idx_val_start_month) + 1) // num_preds_per_fold  \n",
    "\n",
    "        cut_idxs = [idx_val_start_month + i * num_preds_per_fold for i in range(num_folds + 1)]\n",
    "\n",
    "        raw_actuals_val_span = raw_actuals[idx_val_start_month:idx_val_end_month]\n",
    "        raw_actuals_per_fold = [raw_actuals[cut_idxs[i]:cut_idxs[i+1]] for i in range(num_folds)]\n",
    "\n",
    "        if use_rolling_fixed_train_size:\n",
    "            units_per_val_set = num_preds_per_fold\n",
    "        else:\n",
    "            units_per_val_set = 0\n",
    "\n",
    "        train_loader_LIST, X_vals_LIST, Y_vals_LIST = [], [], []\n",
    "\n",
    "        VAL_loss_loader_LIST = [] #### should be one fold ahead of the val set --- there should be no VAL_loss fold for the last set or else there will be leakage !!!\n",
    "                                        ### VAL_loss should be done every two folds , 3 times otal \n",
    "                                        # so VAL_loss with have 8 folds , 6 folds , 4 folds THEN DONE \n",
    "        train_loader_LIST_RAW_Y_VALS = [] \n",
    "        Y_vals_dates_LIST = []\n",
    "\n",
    "        for i in range(num_folds):\n",
    "            X_train = torch.tensor(tensor_formatted_data[      i   *   units_per_val_set      :  cut_idxs[i]]   ).float()\n",
    "            Y_train = torch.tensor(predictor_data_wti_vals[      i   *   units_per_val_set      :  cut_idxs[i]]  ).float()\n",
    "\n",
    "            Y_TRAIN_RAW = torch.tensor(raw_actuals[    i   *   units_per_val_set      :  cut_idxs[i]]  ).float()  ### NEW NEW NEW\n",
    "\n",
    "            X_val = torch.tensor(tensor_formatted_data[cut_idxs[i]:cut_idxs[i+1]]).float()\n",
    "            Y_val = torch.tensor(predictor_data_wti_vals[cut_idxs[i]:cut_idxs[i+1]]).float()\n",
    "\n",
    "            X_VAL_loss = torch.tensor(tensor_formatted_data[cut_idxs[i]:cut_idxs[-1]]).float()\n",
    "            Y_VAL_loss = torch.tensor(predictor_data_wti_vals[cut_idxs[i]:cut_idxs[-1]]).float()\n",
    "            VAL_loss_dates = prediction_date[cut_idxs[i]:cut_idxs[-1]]  ### NEW NEW\n",
    "\n",
    "            # print(VAL_loss_dates)\n",
    "\n",
    "            Y_val_dates = prediction_date[cut_idxs[i]:cut_idxs[i+1]] ### NEW NEW\n",
    "            train_loader_LIST.append(DataLoader(TimeSeriesDataset(X_train, Y_train), batch_size=batch_size, shuffle=False))\n",
    "            train_loader_LIST_RAW_Y_VALS.append(DataLoader(TimeSeriesDataset(X_train, Y_TRAIN_RAW), batch_size=batch_size, shuffle=False))  ### NEW NEW NEW\n",
    "\n",
    "            VAL_loss_loader_LIST.append(DataLoader(TimeSeriesDataset(X_VAL_loss, Y_VAL_loss), batch_size=batch_size, shuffle=False))  ### NEW NEW\n",
    "            \n",
    "            X_vals_LIST.append(X_val)\n",
    "            Y_vals_LIST.append(Y_val)\n",
    "            Y_vals_dates_LIST.append(Y_val_dates) ### NEW NEW\n",
    "\n",
    "    # GRIDSEARCH_FUNCTION_WITH_CV_forTS(**combo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### --------------------------------------- MODEL TRAINING AND EVALUATION  ---------------------------------------\n",
    " \n",
    "\n",
    "        from collections import defaultdict\n",
    "\n",
    "        cv_data = {}  \n",
    "               \n",
    "        model_weight_dict = {f\"combo_number{total_offset + INDEX + 1}\": {\"initial\": {}, \"final\": {}}}  # Initialize the model weight dictionary for the first combo\n",
    "\n",
    "\n",
    "        all_preds = []\n",
    "        all_actuals = []\n",
    "\n",
    "\n",
    "        train_losses_LISTS_dict = {}\n",
    "        VAL_loss_losses_LISTS_dict = {}\n",
    "        \n",
    "        # === Loop over each CV set ===\n",
    "        for set_idx, (train_loader_RAW_Y_vals ,train_loader, VAL_loss_loader, X_val, Y_val) in enumerate(zip(train_loader_LIST_RAW_Y_VALS,train_loader_LIST, VAL_loss_loader_LIST, X_vals_LIST, Y_vals_LIST)):\n",
    "\n",
    "\n",
    "            if is_deterministic:  # --- DETERMINISM BLOCK  ---    ###### NOTE NOTE NOTE thsi must be insdie the loop !!! not outsdie or sle it gets diff weights each time from the RNG\n",
    "                # SEED = 42\n",
    "                SEED = seed_num\n",
    "                random.seed(SEED)\n",
    "                np.random.seed(SEED)\n",
    "                torch.manual_seed(SEED)\n",
    "                torch.cuda.manual_seed_all(SEED)\n",
    "                torch.use_deterministic_algorithms(True)\n",
    "                torch.backends.cudnn.benchmark = False\n",
    "                os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "                torch.backends.cudnn.enabled = True\n",
    "                # --- DETERMINISM BLOCK  ---\n",
    "                # print(f\"Running combo {combo_index} for set {set_idx + 1}\" , flush=True) \n",
    "\n",
    "\n",
    "            if use_class_weighting and use_binary_0_1_retRate and use_dynamic_weights: \n",
    "                y_train_np = torch.cat([y for _, y in train_loader], dim=0).numpy()\n",
    "                num_pos = (y_train_np > 0.5).sum()\n",
    "                num_neg = (y_train_np <= 0.5).sum()\n",
    "                pos_weight_value = (num_neg / num_pos) * POS_weight_multiplier if num_pos > 0 else 1.0\n",
    "                pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "            elif use_class_weighting and (use_binary_neg1_1 or use_ret_rate) and use_dynamic_weights:\n",
    "                y_train_np = torch.cat([y for _, y in train_loader], dim=0).numpy()\n",
    "                num_pos = (y_train_np > 0).sum()\n",
    "                num_neg = (y_train_np <= 0).sum()\n",
    "                pos_weight_value = (num_neg / num_pos) * POS_weight_multiplier if num_pos > 0 else 1.0\n",
    "                pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "\n",
    "            elif use_binary_0_1_retRate_custom_neg and (use_custom_loss_function_BCE_THRESH or use_custom_loss_function_BCE_THRESH_AND_SEVERITY or use_LOW_weights_for_BCE_custom_loss):\n",
    "\n",
    "                length = idx_val_start_month_full -1\n",
    "\n",
    "                num_1 = (raw_actuals_full[:length]    < - binary_0_1_cutoff_ret_rate_percentage).astype(int).sum()\n",
    "                num_0 =  (raw_actuals_full[:length]   > - binary_0_1_cutoff_ret_rate_percentage).astype(int).sum()\n",
    "\n",
    "                pos_weight_value = (num_0/ num_1)   # This will upweight the positive class\n",
    "                pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "\n",
    "            elif use_binary_0_1_retRate_custom_pos and (use_custom_loss_function_BCE_THRESH or use_custom_loss_function_BCE_THRESH_AND_SEVERITY or use_LOW_weights_for_BCE_custom_loss):\n",
    "\n",
    "                length = idx_val_start_month_full -1   \n",
    "\n",
    "                num_1 = (raw_actuals_full[:length]    >  binary_0_1_cutoff_ret_rate_percentage).astype(int).sum()\n",
    "                num_0 =  (raw_actuals_full[:length]   <  binary_0_1_cutoff_ret_rate_percentage).astype(int).sum()\n",
    "\n",
    "                pos_weight_value = (num_0/ num_1)   # This will upweight the positive class\n",
    "                pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "            elif use_class_weighting and use_binary_0_1_retRate_custom_pos:\n",
    "\n",
    "                length = idx_val_start_month_full -1\n",
    "\n",
    "                num_1 = (raw_actuals_full[:length]    >  binary_0_1_cutoff_ret_rate_percentage).astype(int).sum()\n",
    "                num_0 =  (raw_actuals_full[:length]   <  binary_0_1_cutoff_ret_rate_percentage).astype(int).sum()\n",
    "\n",
    "                pos_weight_value = (num_0/ num_1)  * POS_weight_multiplier  # This will upweight the positive class\n",
    "                pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "            elif use_class_weighting and use_binary_0_1_retRate_custom_neg:\n",
    "\n",
    "                length = idx_val_start_month_full -1\n",
    "\n",
    "                num_1 = (raw_actuals_full[:length] < -  binary_0_1_cutoff_ret_rate_percentage).astype(int).sum()\n",
    "                num_0 = (raw_actuals_full[:length]  >  -  binary_0_1_cutoff_ret_rate_percentage).astype(int).sum()\n",
    "\n",
    "                pos_weight_value = (num_0 / num_1) * POS_weight_multiplier  #CHCH  # \n",
    "                pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32).to(device)\n",
    "\n",
    "            else:\n",
    "                pos_weight = None\n",
    "\n",
    "            # print(pos_weight)\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################################################\n",
    "\n",
    "        # for i in range(iter_per_valSET):\n",
    "            # Check if multiple NVIDIA GPUs are available and use DataParallel if so\n",
    "            model = LSTM(input_size, hidden_size, num_layers, use_bidirectional, use_dropout)  # create model instance\n",
    "                                                    ###NEW\n",
    "            if use_existing_initial_weights:\n",
    "\n",
    "                model.load_state_dict(state_dict)\n",
    "                                                            ###NEW\n",
    "            if torch.cuda.is_available() and torch.cuda.device_count() > 1:  # check for multiple NVIDIA GPUs\n",
    "                model = torch.nn.DataParallel(model)  # wrap model to use all available GPUs\n",
    "                print(f\"Using {torch.cuda.device_count()} GPUs: {[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]}\") #*#*#* CHANGED\n",
    "            model = model.to(device)  # move model to the selected device (GPU or CPU)\n",
    "\n",
    "\n",
    "            ##### TESTING: print model architecture                  ##### TESTING: print model architecture\n",
    "\n",
    "            if set_idx == 0:\n",
    "\n",
    "                model_weight_dict[f\"combo_number{total_offset + INDEX + 1}\"][\"initial\"][f\"set_{set_idx + 1}\"] = copy.deepcopy(model.state_dict())   # NOTE NTOE NOTE MUST USE DDEPCOPY HERE OR ELS EHT DATA CHANGED \n",
    "                        \n",
    "            ##### TESTING: print model architecture                ##### TESTING: print model architecture\n",
    "\n",
    "\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            # --- Use weighted loss if requested and binary classification ---\n",
    "            if use_binary_0_1_retRate or use_binary_0_1_retRate_custom_pos or use_binary_0_1_retRate_custom_neg:\n",
    "                if use_class_weighting and pos_weight is not None:\n",
    "                    loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "                else:\n",
    "                    loss_function = nn.BCEWithLogitsLoss()\n",
    "            else:\n",
    "                loss_function = nn.MSELoss()\n",
    "\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "\n",
    "            if (not use_custom_loss_function_BCE_THRESH) and (not use_custom_loss_function_BCE_THRESH_AND_SEVERITY):\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    train_avg_epoch_loss =  train_one_epoch(model, train_loader, optimizer, loss_function) \n",
    "                    train_losses.append(train_avg_epoch_loss)\n",
    "                    # if (set_idx % 2 == 0) and (set_idx < 5 ):  # Validate every two folds and skip the last 4 (since to too little data would be left for reliability)\n",
    "                    if (set_idx < 5 ):  \n",
    "                       val_avg_epoch_loss = validate_one_epoch(model, VAL_loss_loader, loss_function)\n",
    "                       val_losses.append(val_avg_epoch_loss) \n",
    "\n",
    "                train_losses_LISTS_dict[f\"set_{set_idx + 1}\"] = train_losses\n",
    "                VAL_loss_losses_LISTS_dict[f\"set_{set_idx + 1}\"] = val_losses\n",
    "            \n",
    "\n",
    "            if use_custom_loss_function_BCE_THRESH:\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    # print(pos_weight_value)\n",
    "                    train_avg_epoch_loss = train_one_epoch_custom_loss_BCE_THRESH(model, train_loader, optimizer , balancing_Weight_factor = pos_weight_value ,use_LOW_weights = use_LOW_weights_for_BCE_custom_loss  )\n",
    "                    train_losses.append(train_avg_epoch_loss)\n",
    "                    # if (set_idx % 2 == 0) and (set_idx < 5 ):  # Validate every two folds and skip the last 4 (since to too little data would be left for reliability)\n",
    "                    if (set_idx < 5 ):  \n",
    "                       val_avg_epoch_loss = validate_one_epoch_custom_loss_BCE_THRESH(model, VAL_loss_loader, balancing_Weight_factor = pos_weight_value , use_LOW_weights = use_LOW_weights_for_BCE_custom_loss )\n",
    "                       val_losses.append(val_avg_epoch_loss) \n",
    "\n",
    "                train_losses_LISTS_dict[f\"set_{set_idx + 1}\"] = train_losses\n",
    "                VAL_loss_losses_LISTS_dict[f\"set_{set_idx + 1}\"] = val_losses\n",
    "\n",
    "##### RAW val loader needs ot be added, i hace dropped it for now sinc ehte experimentation with this did not show promising results anyway and it is not the in GS\n",
    "\n",
    "            # if use_custom_loss_function_BCE_THRESH_AND_SEVERITY:\n",
    "\n",
    "            #     for epoch in range(num_epochs):\n",
    "            #         train_avg_epoch_loss= train_one_epoch_custom_loss_BCE_THRESH_AND_SEVERITY(model, train_loader, train_loader_RAW_Y_vals, optimizer , balancing_Weight_factor = pos_weight_value ,use_LOW_weights = use_LOW_weights_for_BCE_custom_loss  )\n",
    "            #         train_losses.append(train_avg_epoch_loss)\n",
    "            #         if (set_idx % 2 == 0) and (set_idx < 5 ):  # Validate every two folds and skip the last 4 (since to too little data would be left for reliability)\n",
    "            #            val_avg_epoch_loss = validate_one_epoch_custom_loss_BCE_THRESH_AND_SEVERITY(model, VAL_loss_loader, balancing_Weight_factor = pos_weight_value , use_LOW_weights = use_LOW_weights_for_BCE_custom_loss )\n",
    "            #            val_losses.append(val_avg_epoch_loss) \n",
    "\n",
    "            #     train_losses_LISTS_dict[f\"set_{set_idx + 1}\"] = train_losses\n",
    "            #     val_losses_LISTS_dict[f\"set_{set_idx + 1}\"] = val_losses\n",
    "\n",
    "##### RAW val loader needs ot be added, i hace dropped it for now sinc ehte experimentation with this did not show promising results anyway and it is not the in GS\n",
    "\n",
    "            # print(set_idx , flush=True)\n",
    "            # print(train_losses , flush=True)\n",
    "            # print(val_losses , flush=True)\n",
    "\n",
    "            # print(train_losses_LISTS_dict)\n",
    "            # print(val_losses_LISTS_dict )\n",
    "\n",
    "\n",
    "    # GRIDSEARCH_FUNCTION_WITH_CV_forTS(**combo)\n",
    "\n",
    "\n",
    "            ##### TESTING: print model architecture            ##### TESTING: print model architecture\n",
    "            if set_idx == 0:\n",
    "                model_weight_dict[f\"combo_number{total_offset + INDEX + 1}\"][\"final\"][f\"set_{set_idx + 1}\"] = copy.deepcopy(model.state_dict())  # NOTE NTOE NOTE MUST USE DDEPCOPY HERE OR ELS EHT DATA CHA\n",
    "            ##### TESTING: print model architecture            ##### TESTING: print model architecture\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_output = model(X_val.to(device))\n",
    "                val_predictions = torch.sigmoid(val_output).detach().cpu().numpy().flatten() if (use_binary_0_1_retRate or use_binary_0_1_retRate_custom_neg or use_binary_0_1_retRate_custom_pos) \\\n",
    "                                else val_output.detach().cpu().numpy().flatten()\n",
    "\n",
    "            predicted_array = val_predictions\n",
    "            actual_array = Y_val.numpy().flatten()\n",
    "\n",
    "\n",
    "            all_preds.append(val_predictions)\n",
    "            all_actuals.append(actual_array)\n",
    "\n",
    "            # --- Plot actual vs predicted if true ---\n",
    "            if use_print_acc_vs_pred:\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure(figsize=(8, 2))\n",
    "                plt.axhline(0.5, color='red', linestyle='--', linewidth=1)\n",
    "                plt.plot(actual_array, '.' , label='Actual' )\n",
    "                plt.plot(predicted_array, '.' , label='Predicted')\n",
    "                plt.title(f'Actual vs Predicted (Set {set_idx+1}, Iter {i+1})')\n",
    "                plt.xlabel('Sample')\n",
    "                plt.ylabel('Value')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                # --- Plot actual vs predicted if true ---\n",
    "\n",
    "\n",
    "            if use_binary_0_1_retRate or use_binary_0_1_retRate_custom_neg or use_binary_0_1_retRate_custom_pos:\n",
    "                metrics = evaluate_binary_0_1(predicted_array, actual_array, one_fold= True ,do_print=use_print_acc)\n",
    "            elif use_binary_neg1_1 or use_ret_rate:\n",
    "                metrics = evaluate_signed_neg1_1(predicted_array, actual_array, one_fold= True ,do_print=use_print_acc)\n",
    "\n",
    "\n",
    "            cv_data[f\"set_{set_idx + 1}\"] = metrics\n",
    "\n",
    "\n",
    "        # === Compute overall average ===\n",
    "        metrics_keys = cv_data[f\"set_1\"].keys()\n",
    "\n",
    "        # print(metrics_keys)\n",
    "\n",
    "        overall_avg = {}\n",
    "        for k in metrics_keys:\n",
    "            values = [cv_data[f\"set_{i + 1}\"][k] for i in range(num_folds)]\n",
    "\n",
    "            numeric_values = [v for v in values if v is not None and not (isinstance(v, float) and np.isnan(v)) and isinstance(v, (int, float))]\n",
    "            if len(numeric_values) > 0:\n",
    "                overall_avg[k] = np.mean(numeric_values)\n",
    "\n",
    "            else:\n",
    "                overall_avg[k] = None\n",
    "            # else:\n",
    "            #     # If all values are string messages, keep the message\n",
    "            #     string_values = [v for v in values if isinstance(v, str)]\n",
    "            #     overall_avg[k] = string_values[0] if string_values else np.nan\n",
    "        cv_data[\"avg_across_all_sets\"] = overall_avg\n",
    "        cv_data[\"overall_metrics\"] = evaluate_binary_0_1(all_preds , all_actuals ,one_fold= False ,do_print=False)\n",
    "\n",
    "\n",
    "        ################ POS/NEG THRESHOLD TEST\n",
    "\n",
    "        if  (pred_threshold_sigmoid01_up_bool) and (use_binary_0_1_retRate_custom_neg or use_binary_0_1_retRate_custom_pos):\n",
    "\n",
    "\n",
    "            all_actuals_threshold_per_fold = [] \n",
    "            all_preds_threshold_per_fold = []\n",
    "            for p_fold,a_fold in zip(all_preds , raw_actuals_per_fold):\n",
    "\n",
    "                new_p_fold = []\n",
    "                new_a_fold = []\n",
    "\n",
    "                for p,a in zip(p_fold,a_fold):\n",
    "                    if p > 0.5 and p > pred_threshold_sigmoid01_up:\n",
    "                        new_p_fold.append(p)\n",
    "                        new_a_fold.append(a)\n",
    "\n",
    "                    if p > 0.5 and p < pred_threshold_sigmoid01_up:\n",
    "                            new_p_fold.append('below_threshold')\n",
    "                            new_a_fold.append('below_threshold')\n",
    "\n",
    "                    else:\n",
    "                        new_p_fold.append(p)\n",
    "                        new_a_fold.append(a)\n",
    "\n",
    "\n",
    "                all_preds_threshold_per_fold.append(new_p_fold)\n",
    "                all_actuals_threshold_per_fold.append(new_a_fold)\n",
    "\n",
    "        \n",
    "            all_actuals_threshold_per_fold_flattened = [j for parts in all_actuals_threshold_per_fold for j in parts] \n",
    "            all_preds_threshold_per_fold_flattened = [j for parts in all_preds_threshold_per_fold for j in parts ]\n",
    "            \n",
    "            cv_data[\"overall_metrics_thresh\"] = evaluate_binary_0_1_selective_ensemble(all_preds_threshold_per_fold_flattened , all_actuals_threshold_per_fold_flattened  ,do_print=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        ################ POS/NEG THRESHOLD TEST\n",
    "\n",
    "\n",
    "\n",
    "        # Return with explicit branches (avoid ternary precedence issues)\n",
    "        if pred_threshold_sigmoid01_up_bool:\n",
    "            return (\n",
    "                cv_data, model_weight_dict, all_preds, all_actuals, raw_actuals_per_fold,\n",
    "                all_actuals_threshold_per_fold, all_preds_threshold_per_fold  ,\n",
    "                train_losses_LISTS_dict , VAL_loss_losses_LISTS_dict\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                cv_data, model_weight_dict, all_preds, all_actuals, raw_actuals_per_fold , Y_vals_dates_LIST ,\n",
    "                train_losses_LISTS_dict , VAL_loss_losses_LISTS_dict\n",
    "            )\n",
    "\n",
    "\n",
    "        # return cv_data , model_weight_dict  , all_preds , all_actuals ,raw_actuals_per_fold , \\\n",
    "        #         all_actuals_threshold_per_fold , all_preds_threshold_per_fold if pred_threshold_sigmoid01_up_bool  \\\n",
    "        #         else cv_data , model_weight_dict  , all_preds , all_actuals ,raw_actuals_per_fold  \\\n",
    "    \n",
    "\n",
    "\n",
    "    if  not pred_threshold_sigmoid01_up_bool:\n",
    "\n",
    "        cv_data , model_weight_dict , all_preds , all_actuals , raw_actuals_per_fold , Y_vals_dates_LIST , \\\n",
    "        train_losses_LISTS_dict , VAL_loss_losses_LISTS_dict = GRIDSEARCH_FUNCTION_WITH_CV_forTS(**combo)\n",
    "\n",
    "        result_entry = {\n",
    "        \"combo_number\": total_offset + INDEX + 1,\n",
    "        \"parameters\": combo,\n",
    "        \"cv_sets\": cv_data,\n",
    "        \"all_preds\" : all_preds ,\n",
    "        \"all_actuals\" : all_actuals,\n",
    "        \"raw_actuals\" : raw_actuals_per_fold ,\n",
    "        \"Y_vals_dates_LIST\" : Y_vals_dates_LIST , \n",
    "        'train_losses_LISTS_dict' :train_losses_LISTS_dict ,\n",
    "        'val_losses_LISTS_dict' : VAL_loss_losses_LISTS_dict\n",
    "        \n",
    "    }\n",
    "\n",
    "    else:\n",
    "        cv_data , model_weight_dict , all_preds , all_actuals , raw_actuals_per_fold , all_actuals_threshold_per_fold , \\\n",
    "        all_preds_threshold_per_fold , train_losses_LISTS_dict , VAL_loss_losses_LISTS_dict = GRIDSEARCH_FUNCTION_WITH_CV_forTS(**combo)\n",
    "\n",
    "        result_entry = {\n",
    "        \"combo_number\": total_offset + INDEX + 1,\n",
    "        \"parameters\": combo,\n",
    "        \"cv_sets\": cv_data,\n",
    "        \"all_preds\" : all_preds ,\n",
    "        \"all_actuals\" : all_actuals,\n",
    "        \"raw_actuals\" : raw_actuals_per_fold ,\n",
    "        \"all_actuals_threshold_per_fold\" :all_actuals_threshold_per_fold , \n",
    "        \"all_preds_threshold_per_fold\" : all_preds_threshold_per_fold ,\n",
    "        'train_losses_LISTS_dict' :train_losses_LISTS_dict ,\n",
    "        'val_losses_LISTS_dict' : VAL_loss_losses_LISTS_dict\n",
    "        \n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"--- Running Combo {total_offset + INDEX + 1} ---\" \n",
    "        f\"Parameters: {combo}\\n\" ,\n",
    "        # f\"â†’ Fold Accuracies: \" +\n",
    "        # \"  \".join(\n",
    "        #     f\"{k}: {cv_data[k][0]['accuracy']}\"  # <-- each set is now a list of metric dicts\n",
    "        #     for k in sorted(cv_data) if k.startswith(\"set_\")\n",
    "        # ) +\n",
    "        # f\"\\nâ†’ Overall Avg Accuracy: {cv_data['avg_across_all_sets']['accuracy']}\",\n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "\n",
    "    return result_entry ,model_weight_dict\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5603537c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Combo 5 ---Parameters: {'binary_0_1_cutoff_ret_rate_percentage': 0.1, 'learning_rate': 0.005, 'num_epochs': 10, 'batch_size': 50, 'use_bidirectional': False, 'lag': 3, 'input_size': 12, 'hidden_size': 35, 'num_layers': 6, 'use_monthly_dfs_only': True, 'use_binary_0_1_retRate': False, 'use_custom_loss_function_BCE_THRESH': True, 'use_custom_loss_function_BCE_THRESH_AND_SEVERITY': False, 'use_LOW_weights_for_BCE_custom_loss': False, 'pred_threshold_sigmoid01_up': None, 'use_binary_neg1_1': False, 'use_ret_rate': False, 'use_print_acc': False, 'use_dropout': False, 'use_class_weighting': False, 'is_deterministic': True, 'seed_num': 36898, 'use_existing_lagged_data': True, 'use_dynamic_weights': False, 'use_binary_0_1_retRate_custom_neg': False, 'use_binary_0_1_retRate_custom_pos': True, 'POS_weight_multiplier': 1.5, 'use_rolling_fixed_train_size': False, 'use_existing_initial_weights': False, 'state_dict': None, 'use_USO_wticoncat_predictor_WEEKLY_END_MO': False, 'use_UCO_wticoncat_predictor_WEEKLY_END_MO': False, 'use_HUC_wticoncat_predictor_WEEKLY_END_MO': False, 'use_HOD_wticoncat_predictor_WEEKLY_END_MO': True, 'use_CRUD_wticoncat_predictor_WEEKLY_END_MO': False, 'use_SCO_wticoncat_predictor_WEEKLY_END_MO': False, 'train_start_month': '2004-01', 'val_start_month': '2020-01', 'val_end_month': '2021-12', 'num_preds_per_fold': 3}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "cc = {'binary_0_1_cutoff_ret_rate_percentage': 0.1, 'learning_rate': 0.005, 'num_epochs': 10, 'batch_size': 50, 'use_bidirectional': False, 'lag': 3, 'input_size': 12, 'hidden_size': 35,\n",
    "       'num_layers': 6, 'use_monthly_dfs_only': True, 'use_binary_0_1_retRate': False, 'use_custom_loss_function_BCE_THRESH': True, 'use_custom_loss_function_BCE_THRESH_AND_SEVERITY': False, \n",
    "       'use_LOW_weights_for_BCE_custom_loss': False, 'pred_threshold_sigmoid01_up': None, 'use_binary_neg1_1': False, 'use_ret_rate': False, 'use_print_acc': False, 'use_dropout': False, \n",
    "       'use_class_weighting': False, 'is_deterministic': True, 'seed_num': 36898, 'use_existing_lagged_data': True, 'use_dynamic_weights': False, 'use_binary_0_1_retRate_custom_neg': False,\n",
    "        'use_binary_0_1_retRate_custom_pos': True, 'POS_weight_multiplier': 1.5, 'use_rolling_fixed_train_size': False, 'use_existing_initial_weights': False, 'state_dict': None, \n",
    "        'use_USO_wticoncat_predictor_WEEKLY_END_MO': False, 'use_UCO_wticoncat_predictor_WEEKLY_END_MO': False, 'use_HUC_wticoncat_predictor_WEEKLY_END_MO': False, 'use_HOD_wticoncat_predictor_WEEKLY_END_MO': True,\n",
    "        'use_CRUD_wticoncat_predictor_WEEKLY_END_MO': False, 'use_SCO_wticoncat_predictor_WEEKLY_END_MO': False, 'train_start_month': '2004-01', 'val_start_month': '2020-01', 'val_end_month': '2021-12', \n",
    "        'num_preds_per_fold': 3}\n",
    "\n",
    "r,w = run_combo_V_5(0, cc, 4, use_print_acc_vs_pred=False , pred_threshold_sigmoid01_up_bool = False )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8bb55b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'combo_number': 5,\n",
       " 'parameters': {'binary_0_1_cutoff_ret_rate_percentage': 0.1,\n",
       "  'learning_rate': 0.005,\n",
       "  'num_epochs': 10,\n",
       "  'batch_size': 50,\n",
       "  'use_bidirectional': False,\n",
       "  'lag': 3,\n",
       "  'input_size': 12,\n",
       "  'hidden_size': 35,\n",
       "  'num_layers': 6,\n",
       "  'use_monthly_dfs_only': True,\n",
       "  'use_binary_0_1_retRate': False,\n",
       "  'use_custom_loss_function_BCE_THRESH': True,\n",
       "  'use_custom_loss_function_BCE_THRESH_AND_SEVERITY': False,\n",
       "  'use_LOW_weights_for_BCE_custom_loss': False,\n",
       "  'pred_threshold_sigmoid01_up': None,\n",
       "  'use_binary_neg1_1': False,\n",
       "  'use_ret_rate': False,\n",
       "  'use_print_acc': False,\n",
       "  'use_dropout': False,\n",
       "  'use_class_weighting': False,\n",
       "  'is_deterministic': True,\n",
       "  'seed_num': 36898,\n",
       "  'use_existing_lagged_data': True,\n",
       "  'use_dynamic_weights': False,\n",
       "  'use_binary_0_1_retRate_custom_neg': False,\n",
       "  'use_binary_0_1_retRate_custom_pos': True,\n",
       "  'POS_weight_multiplier': 1.5,\n",
       "  'use_rolling_fixed_train_size': False,\n",
       "  'use_existing_initial_weights': False,\n",
       "  'state_dict': None,\n",
       "  'use_USO_wticoncat_predictor_WEEKLY_END_MO': False,\n",
       "  'use_UCO_wticoncat_predictor_WEEKLY_END_MO': False,\n",
       "  'use_HUC_wticoncat_predictor_WEEKLY_END_MO': False,\n",
       "  'use_HOD_wticoncat_predictor_WEEKLY_END_MO': True,\n",
       "  'use_CRUD_wticoncat_predictor_WEEKLY_END_MO': False,\n",
       "  'use_SCO_wticoncat_predictor_WEEKLY_END_MO': False,\n",
       "  'train_start_month': '2004-01',\n",
       "  'val_start_month': '2020-01',\n",
       "  'val_end_month': '2021-12',\n",
       "  'num_preds_per_fold': 3},\n",
       " 'cv_sets': {'set_1': {'accuracy': np.float64(0.0),\n",
       "   'precision_up': None,\n",
       "   'recall_up': 0,\n",
       "   'precision_down': 0,\n",
       "   'recall_down': None},\n",
       "  'set_2': {'accuracy': np.float64(66.66666666666666),\n",
       "   'precision_up': None,\n",
       "   'recall_up': 0,\n",
       "   'precision_down': np.float64(66.66666666666666),\n",
       "   'recall_down': np.float64(100.0)},\n",
       "  'set_3': {'accuracy': np.float64(100.0),\n",
       "   'precision_up': None,\n",
       "   'recall_up': None,\n",
       "   'precision_down': np.float64(100.0),\n",
       "   'recall_down': np.float64(100.0)},\n",
       "  'set_4': {'accuracy': np.float64(100.0),\n",
       "   'precision_up': None,\n",
       "   'recall_up': None,\n",
       "   'precision_down': np.float64(100.0),\n",
       "   'recall_down': np.float64(100.0)},\n",
       "  'set_5': {'accuracy': np.float64(100.0),\n",
       "   'precision_up': None,\n",
       "   'recall_up': None,\n",
       "   'precision_down': np.float64(100.0),\n",
       "   'recall_down': np.float64(100.0)},\n",
       "  'set_6': {'accuracy': np.float64(100.0),\n",
       "   'precision_up': None,\n",
       "   'recall_up': None,\n",
       "   'precision_down': np.float64(100.0),\n",
       "   'recall_down': np.float64(100.0)},\n",
       "  'set_7': {'accuracy': np.float64(100.0),\n",
       "   'precision_up': None,\n",
       "   'recall_up': None,\n",
       "   'precision_down': np.float64(100.0),\n",
       "   'recall_down': np.float64(100.0)},\n",
       "  'set_8': {'accuracy': np.float64(66.66666666666666),\n",
       "   'precision_up': None,\n",
       "   'recall_up': 0,\n",
       "   'precision_down': np.float64(66.66666666666666),\n",
       "   'recall_down': np.float64(100.0)},\n",
       "  'avg_across_all_sets': {'accuracy': np.float64(79.16666666666666),\n",
       "   'precision_up': None,\n",
       "   'recall_up': np.float64(0.0),\n",
       "   'precision_down': np.float64(79.16666666666666),\n",
       "   'recall_down': np.float64(100.0)},\n",
       "  'overall_metrics': {'accuracy': np.float64(79.16666666666666),\n",
       "   'precision_up': None,\n",
       "   'recall_up': 0,\n",
       "   'precision_down': np.float64(79.16666666666666),\n",
       "   'recall_down': np.float64(100.0)}},\n",
       " 'all_preds': [array([0.4924872 , 0.49248603, 0.4924862 ], dtype=float32),\n",
       "  array([0.49395555, 0.49395695, 0.49395856], dtype=float32),\n",
       "  array([0.49367777, 0.49367568, 0.49367568], dtype=float32),\n",
       "  array([0.44208708, 0.44208995, 0.44209078], dtype=float32),\n",
       "  array([0.44209394, 0.44209334, 0.44209316], dtype=float32),\n",
       "  array([0.44209352, 0.44209316, 0.4420939 ], dtype=float32),\n",
       "  array([0.4420931 , 0.44209284, 0.4420939 ], dtype=float32),\n",
       "  array([0.44209418, 0.44209352, 0.44209328], dtype=float32)],\n",
       " 'all_actuals': [array([1., 1., 1.], dtype=float32),\n",
       "  array([1., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0.], dtype=float32),\n",
       "  array([0., 0., 0.], dtype=float32),\n",
       "  array([0., 1., 0.], dtype=float32)],\n",
       " 'raw_actuals': [array([0.2775842 , 0.14787879, 1.40681098]),\n",
       "  array([ 0.30975101, -0.28113223, -0.13350419]),\n",
       "  array([-0.05189567, -0.04608622,  0.06451613]),\n",
       "  array([ 0.03183913, -0.18013263, -0.11191812]),\n",
       "  array([-0.13104089, -0.28770053, -0.01861862]),\n",
       "  array([-0.12913097, -0.0983837 , -0.22057677]),\n",
       "  array([-0.031375  ,  0.08117176, -0.18214371]),\n",
       "  array([-0.22387624,  0.19405792, -0.09700787])],\n",
       " 'Y_vals_dates_LIST': [DatetimeIndex(['2020-01-31', '2020-02-29', '2020-03-31'], dtype='datetime64[ns]', freq=None),\n",
       "  DatetimeIndex(['2020-04-30', '2020-05-31', '2020-06-30'], dtype='datetime64[ns]', freq=None),\n",
       "  DatetimeIndex(['2020-07-31', '2020-08-31', '2020-09-30'], dtype='datetime64[ns]', freq=None),\n",
       "  DatetimeIndex(['2020-10-31', '2020-11-30', '2020-12-31'], dtype='datetime64[ns]', freq=None),\n",
       "  DatetimeIndex(['2021-01-31', '2021-02-28', '2021-03-31'], dtype='datetime64[ns]', freq=None),\n",
       "  DatetimeIndex(['2021-04-30', '2021-05-31', '2021-06-30'], dtype='datetime64[ns]', freq=None),\n",
       "  DatetimeIndex(['2021-07-31', '2021-08-31', '2021-09-30'], dtype='datetime64[ns]', freq=None),\n",
       "  DatetimeIndex(['2021-10-31', '2021-11-30', '2021-12-31'], dtype='datetime64[ns]', freq=None)],\n",
       " 'train_losses_LISTS_dict': {'set_1': [1.06484854221344,\n",
       "   1.0616971850395203,\n",
       "   1.060945451259613,\n",
       "   1.0606889724731445,\n",
       "   1.0606359392404556,\n",
       "   1.0606407672166824,\n",
       "   1.0606425255537033,\n",
       "   1.0606299340724945,\n",
       "   1.0606104135513306,\n",
       "   1.0605917423963547],\n",
       "  'set_2': [1.0916684567928314,\n",
       "   1.0858839452266693,\n",
       "   0.9918756186962128,\n",
       "   1.0183935910463333,\n",
       "   1.0845208764076233,\n",
       "   1.0842996835708618,\n",
       "   1.0839303880929947,\n",
       "   0.9918891042470932,\n",
       "   1.018148198723793,\n",
       "   1.0843497663736343],\n",
       "  'set_3': [1.0950209498405457,\n",
       "   1.088805541396141,\n",
       "   0.9921762496232986,\n",
       "   1.0215687602758408,\n",
       "   1.0872882604599,\n",
       "   1.0870229750871658,\n",
       "   1.0866077840328217,\n",
       "   0.913082167506218,\n",
       "   1.0874593704938889,\n",
       "   1.0873877108097076],\n",
       "  'set_4': [1.0013086915016174,\n",
       "   0.9970786690711975,\n",
       "   0.9959175825119019,\n",
       "   0.995238745212555,\n",
       "   0.9948615550994873,\n",
       "   0.9946854114532471,\n",
       "   0.9946306467056274,\n",
       "   0.9946372747421265,\n",
       "   0.9946660280227662,\n",
       "   0.9946951150894165],\n",
       "  'set_5': [1.0013087272644043,\n",
       "   0.9970786929130554,\n",
       "   0.9959176421165467,\n",
       "   0.9952388644218445,\n",
       "   0.9948617219924927,\n",
       "   0.9946856379508973,\n",
       "   0.9946310520172119,\n",
       "   0.9946379780769348,\n",
       "   0.9946673035621643,\n",
       "   0.9946976900100708],\n",
       "  'set_6': [1.0013087391853333,\n",
       "   0.9970786690711975,\n",
       "   0.9959176182746887,\n",
       "   0.9952388882637024,\n",
       "   0.9948617100715638,\n",
       "   0.9946856737136841,\n",
       "   0.9946310520172119,\n",
       "   0.9946379899978638,\n",
       "   0.9946673274040222,\n",
       "   0.9946977257728576],\n",
       "  'set_7': [1.0013087511062622,\n",
       "   0.9970786929130554,\n",
       "   0.9959176182746887,\n",
       "   0.9952388525009155,\n",
       "   0.9948616862297058,\n",
       "   0.9946856260299682,\n",
       "   0.994631004333496,\n",
       "   0.9946379423141479,\n",
       "   0.9946672558784485,\n",
       "   0.9946975708007812],\n",
       "  'set_8': [1.001308763027191,\n",
       "   0.9970786452293396,\n",
       "   0.9959176540374756,\n",
       "   0.9952388525009155,\n",
       "   0.9948617100715638,\n",
       "   0.9946856141090393,\n",
       "   0.994631004333496,\n",
       "   0.9946379661560059,\n",
       "   0.9946672439575195,\n",
       "   0.9946976780891419]},\n",
       " 'val_losses_LISTS_dict': {'set_1': [1.0360450744628906,\n",
       "   1.0362768173217773,\n",
       "   1.0367282629013062,\n",
       "   1.0371757745742798,\n",
       "   1.0374878644943237,\n",
       "   1.037629246711731,\n",
       "   1.037634015083313,\n",
       "   1.0375592708587646,\n",
       "   1.037455439567566,\n",
       "   1.0373538732528687],\n",
       "  'set_2': [0.8234286308288574,\n",
       "   0.8409045934677124,\n",
       "   0.6946126222610474,\n",
       "   0.8443949818611145,\n",
       "   0.8431786894798279,\n",
       "   0.8453524708747864,\n",
       "   0.8494620323181152,\n",
       "   0.6947898268699646,\n",
       "   0.8458368182182312,\n",
       "   0.8440612554550171],\n",
       "  'set_3': [0.7492392063140869,\n",
       "   0.7738661170005798,\n",
       "   0.6982808709144592,\n",
       "   0.7793050408363342,\n",
       "   0.7778551578521729,\n",
       "   0.7810682654380798,\n",
       "   0.6951346397399902,\n",
       "   0.7785372734069824,\n",
       "   0.7747070789337158,\n",
       "   0.7757602334022522],\n",
       "  'set_4': [0.7662964463233948,\n",
       "   0.7592852115631104,\n",
       "   0.7522314190864563,\n",
       "   0.7460646033287048,\n",
       "   0.7409612536430359,\n",
       "   0.736928403377533,\n",
       "   0.7338821887969971,\n",
       "   0.7316853404045105,\n",
       "   0.7301761507987976,\n",
       "   0.7291930317878723],\n",
       "  'set_5': [0.798031747341156,\n",
       "   0.7918528914451599,\n",
       "   0.7856590747833252,\n",
       "   0.7802636027336121,\n",
       "   0.7758131623268127,\n",
       "   0.7723061442375183,\n",
       "   0.7696633338928223,\n",
       "   0.7677609324455261,\n",
       "   0.7664563059806824,\n",
       "   0.7656084895133972],\n",
       "  'set_6': [],\n",
       "  'set_7': [],\n",
       "  'set_8': []}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870a8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45b9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835bcbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7082a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83834c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3dc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19fa8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19998f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ea807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06090fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced77a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce0f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02061c3b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_1",
   "language": "python",
   "name": "venv_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
